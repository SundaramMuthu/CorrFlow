<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Self-supervised Learning for Video Correspondence Flow</title>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">

	<!-- Loading Bootstrap -->
	<link href="css/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

	<!-- Latest compiled and minified CSS -->
	<link rel="stylesheet" href="css/site.css">
	<link rel="shortcut icon" href="img/favicon.ico">

	<style type="text/css">
	.video-responsive{
		overflow:hidden;
		padding-bottom:56.25%;
		position:relative;
		height:0;
	}
	.video-responsive iframe{
		left:0;
		top:0;
		height:100%;
		width:100%;
		position:absolute;
	}
	</style>

	<!-- HTML5 shim, for IE6-8 support of HTML5 elements. All other JS at the end of file. -->
    <!--[if lt IE 9]>
      <script src="js/vendor/html5shiv.js"></script>
      <script src="js/vendor/respond.min.js"></script>
  <![endif]-->
</head>
<body>

	<div class="container">
		<div class="">
			<h3 class="text-center">Self-supervised Learning for Video Correspondence Flow</h3>
			<p class="text-center">Zihang Lai<sup>1</sup>, Weidi Xie<sup>2</sup></p>
			<p class="text-center">BMVC 2019 (Oral)</p>
			<br>
			<p class="text-center"><sup>1</sup>Department of Computer Science, University of Oxford </p>
			<p class="text-center"><sup>2</sup>Visual Geometry Group, Department of Engineering Science, University of Oxford </p>

		</div>

		<div class="container col-sm-offset-1 col-sm-10">
			<div class="row">
				<div class="col-xs-offset-2 col-xs-8 col-sm-offset-0 col-sm-4 col-md-4">
					<img class="img-responsive" src="img/img1.gif">
				</div>
				<div class="col-xs-offset-2 col-xs-8 col-sm-offset-0 col-sm-4 col-md-4">
					<img class="img-responsive" src="img/img2.gif">
				</div>
				<div class="col-xs-offset-2 col-xs-8 col-sm-offset-0 col-sm-4 col-md-4">
					<img class="img-responsive" src="img/img3.gif">
				</div>
			</div>
			<br>

			<div class="row">
				<div class="col-xs-offset-2 col-xs-8 col-sm-offset-0 col-sm-4 col-md-4">
					<img class="img-responsive" src="img/img4.gif">
				</div>
				<div class="col-xs-offset-2 col-xs-8 col-sm-offset-0 col-sm-4 col-md-4">
					<img class="img-responsive" src="img/img5.gif">
				</div>
				<div class="col-xs-offset-2 col-xs-8 col-sm-offset-0 col-sm-4 col-md-4">
					<img class="img-responsive" src="img/img6.gif">
				</div>
			</div>
			<p class="text-center">DAVIS Semi-supervised prediction task (Given first frame)</p>

		</div>
		<div class="col-md-10 col-md-offset-1">

			<div class="panel panel-default">
				<div class="panel-heading">Abstract</div>

				<div class="panel-body">
					<p class="small">The objective of this paper is self-supervised learning of feature embeddings that are suitable for matching correspondences along the videos, which we term correspondence flow. By leveraging the natural spatial-temporal coherence in videos, we propose to train a “pointer” that reconstructs a target frame by copying pixels from a reference frame. 

						<br>We make the following contributions: First, we introduce a simple information bottleneck that forces the model to learn robust features for correspondence matching, and prevent it from learning trivial solutions, e.g. matching based on low-level colour information. Second, to tackle the challenges from tracker drifting, due to complex object deformations, illumination changes and occlusions, we propose to train a recursive model over long temporal windows with scheduled sampling and cycle consistency. Third, we achieve state-of-the-art performance on DAVIS 2017 video segmentation and JHMDB keypoint tracking tasks, outperforming all previous self-supervised learning approaches by a significant margin. Fourth, in order to shed light on the potential of self-supervised learning on the task of video correspondence flow, we probe the upper bound by training on additional data, i.e. more diverse videos, further demonstrating
significant improvements on video segmentation. 
					</p>
					<pre>@inproceedings{Lai19,
  title={Self-supervised Learning for Video Correspondence Flow},
  author={Lai, Z. and Xie, W.},
  booktitle={BMVC},
  year={2019}
}
</pre>
				</div>
			</div>

			<div class="panel panel-default">
				<div class="panel-heading">Embedding PCA color illustration</div>

				<div class="panel-body">
		<div class="container col-sm-12">
			<div class="row">
				<div class="col-xs-offset-2 col-xs-8 col-sm-offset-0 col-sm-4 col-md-4">
					<img class="img-responsive" src="img/pca1a.gif">
				</div>
				<div class="col-xs-offset-2 col-xs-8 col-sm-offset-0 col-sm-4 col-md-4">
					<img class="img-responsive" src="img/pca2a.gif">
				</div>
				<div class="col-xs-offset-2 col-xs-8 col-sm-offset-0 col-sm-4 col-md-4">
					<img class="img-responsive" src="img/pca3a.gif">
				</div>
			</div>
			<br>

			<div class="row">
				<div class="col-xs-offset-2 col-xs-8 col-sm-offset-0 col-sm-4 col-md-4">
					<img class="img-responsive" src="img/pca1b.gif">
				</div>
				<div class="col-xs-offset-2 col-xs-8 col-sm-offset-0 col-sm-4 col-md-4">
					<img class="img-responsive" src="img/pca2b.gif">
				</div>
				<div class="col-xs-offset-2 col-xs-8 col-sm-offset-0 col-sm-4 col-md-4">
					<img class="img-responsive" src="img/pca3b.gif">
				</div>
			</div>

		</div>
				</div>
			</div>

			<div class="panel panel-default">
				<div class="panel-heading">Downloads</div>

				<div class="panel-body">
					<ul>
						<li><b>Paper: </b> <a href="https://arxiv.org/pdf/1905.00875.pdf">ArXiv</a></li>
						<li><b>Code + Pretrained model: </b> <a href="https://github.com/zlai0/CorrFlow">GitHub</a></li>
						<li><b>Presentation: </b> <a href="https://drive.google.com/open?id=1ol8RixtNvRf3M1lkhj8o0wbwSQPRKjgY">Google Drive</a></li>
						<li><b>Poster: </b> <a href="https://drive.google.com/open?id=19tr6ub--G96aTStD1WTFQvvRQMoKk2oY">Google Drive</a></li>
						<li><b>Dataset: </b> 
							<a href="https://deepmind.com/research/open-source/open-source-datasets/kinetics/">Kinetics</a>, 
							<a href="https://davischallenge.org/davis2017/code.html">DAVIS-2017</a>,
							<a href="https://oxuva.github.io/long-term-tracking-benchmark/">OxUvA</a>
						</li>
					</ul>
					Please contact zihang.lai at cs.ox.ac.uk if you have any questions.
				</div>
			</div>		
			<div class="panel panel-default">
				<div class="panel-heading">Results</div>

				<div class="panel-body">
					<img class="img-responsive" src="img/results1.png">
					<p class="text-center"> Video segmentation results on DAVIS-2017 dataset. Higher values are better. </p>

					<img class="img-responsive" src="img/results2.png">
					<p class="text-center">Accuracy by attributes.</p>

				</div>
			</div>
		
			<div class="panel panel-default">
				<div class="panel-heading"><b>Acknowledgements</b></div>

				<div class="panel-body">
We gratefully acknowledge the support of the EPSRC Programme Grant <a href="http://www.robots.ox.ac.uk/~vgg/projects/seebibyte/">Seebibyte EP/M013774/1: Visual Search for the Era of Big Data</a>.

				</div>
			</div>		

		<!-- jQuery (necessary for Flat UI's JavaScript plugins) -->
		<script src="js/vendor/jquery.min.js"></script>
	</body>
	</html>
